{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MARCO4: Per-Cell Logits Architecture for ARC-AGI\n",
    "\n",
    "This notebook demonstrates the MARCO4 system using:\n",
    "- **Per-cell logits** from LLM experts (probabilities for colors 0-9)\n",
    "- **Dempster-Shafer theory** for evidence combination across experts\n",
    "- **MCU-driven branching** for uncertain cells\n",
    "- **A* search** through Cognitive State Space\n",
    "\n",
    "## Models Available\n",
    "- `models/qwen` - Qwen model (BitsAndBytes 4-bit)\n",
    "- `models/phi3` - Phi-3 model (BitsAndBytes 4-bit)\n",
    "- `models/gpt-oss` - GPT-OSS model (Mxfp4 pre-quantized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import time\n",
    "\n",
    "# Add MARCO4 to path\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "# MARCO4 imports\n",
    "from marco import (\n",
    "    MCU, Expert, MARCO4Config,\n",
    "    dempster_combine, compute_belief, compute_plausibility,\n",
    "    token_probs_to_belief_mass, THETA,\n",
    "    create_empty_grid, is_complete_grid, grid_to_string,\n",
    "    ARCProblem, solve_task, evaluate_solution,\n",
    "    CognitiveStateSpace, BranchStatus\n",
    ")\n",
    "\n",
    "print(\"MARCO4 imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, BitsAndBytesConfig\n",
    "import gc\n",
    "\n",
    "# Models that are already quantized (don't apply additional quantization)\n",
    "# These models have quantization baked into their config.json\n",
    "PREQUANTIZED_MODELS = {'gpt-oss'}\n",
    "\n",
    "# Models that require bfloat16 instead of float16\n",
    "BFLOAT16_MODELS = {'gpt-oss'}\n",
    "\n",
    "class LLMManager:\n",
    "    \"\"\"Manages multiple LLM models for multi-expert system.\"\"\"\n",
    "    \n",
    "    def __init__(self, device: str = \"auto\", use_4bit: bool = True):\n",
    "        self.device = device if device != \"auto\" else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.use_4bit = use_4bit and self.device == \"cuda\"\n",
    "        self.models = {}       # model_name -> model\n",
    "        self.tokenizers = {}   # model_name -> tokenizer\n",
    "        print(f\"LLMManager initialized with device: {self.device}\")\n",
    "    \n",
    "    def load_model(self, model_path: str, model_name: str = None):\n",
    "        \"\"\"Load a model from path.\"\"\"\n",
    "        model_name = model_name or Path(model_path).name\n",
    "        print(f\"\\nLoading {model_name} from: {model_path}\")\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        # Check if model is already quantized (e.g., gpt-oss with Mxfp4Config)\n",
    "        is_prequantized = model_name in PREQUANTIZED_MODELS\n",
    "        requires_bfloat16 = model_name in BFLOAT16_MODELS\n",
    "        \n",
    "        # For pre-quantized models, check if the config has quantization info\n",
    "        if is_prequantized:\n",
    "            try:\n",
    "                config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n",
    "                quant_info = getattr(config, 'quantization_config', None)\n",
    "                if quant_info:\n",
    "                    quant_method = quant_info.get('quant_method', 'unknown') if isinstance(quant_info, dict) else 'custom'\n",
    "                    print(f\"  Model has built-in quantization: {quant_method}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Could not read config: {e}\")\n",
    "        \n",
    "        # Determine dtype\n",
    "        if self.device == \"cuda\":\n",
    "            if requires_bfloat16:\n",
    "                torch_dtype = torch.bfloat16\n",
    "                print(\"  Using bfloat16 dtype\")\n",
    "            else:\n",
    "                torch_dtype = torch.float16\n",
    "        else:\n",
    "            torch_dtype = torch.float32\n",
    "        \n",
    "        # Build loading kwargs\n",
    "        load_kwargs = {\n",
    "            'device_map': \"auto\" if self.device == \"cuda\" else None,\n",
    "            'torch_dtype': torch_dtype,\n",
    "            'trust_remote_code': True,\n",
    "            'low_cpu_mem_usage': True,\n",
    "        }\n",
    "        \n",
    "        # Only apply BitsAndBytes quantization to non-prequantized models\n",
    "        if self.use_4bit and not is_prequantized:\n",
    "            load_kwargs['quantization_config'] = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\"\n",
    "            )\n",
    "            print(\"  Using 4-bit quantization (BitsAndBytes)\")\n",
    "        elif is_prequantized:\n",
    "            # Don't pass quantization_config - let model use its built-in config\n",
    "            print(\"  Loading with built-in quantization config\")\n",
    "        \n",
    "        # Load model\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_path, **load_kwargs)\n",
    "        \n",
    "        if self.device == \"cpu\" and not is_prequantized:\n",
    "            model = model.to(self.device)\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        # Store\n",
    "        self.models[model_name] = model\n",
    "        self.tokenizers[model_name] = tokenizer\n",
    "        \n",
    "        params = sum(p.numel() for p in model.parameters()) / 1e9\n",
    "        print(f\"  ✓ {model_name} loaded: {params:.2f}B parameters\")\n",
    "        \n",
    "        return model_name\n",
    "    \n",
    "    def get_model(self, model_name: str):\n",
    "        \"\"\"Get a loaded model by name.\"\"\"\n",
    "        return self.models.get(model_name), self.tokenizers.get(model_name)\n",
    "    \n",
    "    def list_models(self):\n",
    "        \"\"\"List all loaded models.\"\"\"\n",
    "        return list(self.models.keys())\n",
    "    \n",
    "    def unload(self, model_name: str = None):\n",
    "        \"\"\"Unload model(s) to free memory.\"\"\"\n",
    "        if model_name:\n",
    "            if model_name in self.models:\n",
    "                del self.models[model_name]\n",
    "                del self.tokenizers[model_name]\n",
    "                print(f\"Unloaded {model_name}\")\n",
    "        else:\n",
    "            # Unload all\n",
    "            self.models.clear()\n",
    "            self.tokenizers.clear()\n",
    "            print(\"All models unloaded\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "# Initialize manager\n",
    "llm_manager = LLMManager(device=\"auto\", use_4bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model paths\n",
    "MODEL_PATHS = {\n",
    "    'qwen': '../models/qwen',\n",
    "    'phi3': '../models/phi3',\n",
    "    'gpt-oss': '../models/gpt-oss'\n",
    "}\n",
    "\n",
    "# Load ALL models for multi-expert system\n",
    "# Each expert will use a different model for true diversity\n",
    "loaded_models = []\n",
    "for name, path in MODEL_PATHS.items():\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            llm_manager.load_model(path, name)\n",
    "            loaded_models.append(name)\n",
    "        except Exception as e:\n",
    "            print(f\"  Failed to load {name}: {e}\")\n",
    "    else:\n",
    "        print(f\"  Model path not found: {path}\")\n",
    "\n",
    "print(f\"\\nLoaded {len(loaded_models)} models: {loaded_models}\")\n",
    "\n",
    "if len(loaded_models) == 0:\n",
    "    raise RuntimeError(\"No models could be loaded. Check model paths.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create LLM-Based Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from marco.expert import Expert, CellLogits\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TransformerExpert(Expert):\n",
    "    \"\"\"\n",
    "    Expert that uses a transformer model to output per-cell logits.\n",
    "    \n",
    "    For each cell, outputs log-probabilities for colors 0-9.\n",
    "    No augmentation - single forward pass per cell.\n",
    "    \n",
    "    Pipeline:\n",
    "    1. For each cell position, create a prompt asking for the cell value\n",
    "    2. Get logits for tokens '0'-'9' from the LLM\n",
    "    3. Apply log_softmax to get log-probabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, expert_id: str, llm_manager: LLMManager, model_name: str,\n",
    "                 config=None):\n",
    "        super().__init__(expert_id, config)\n",
    "        self.llm_manager = llm_manager\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        self.model, self.tokenizer = llm_manager.get_model(model_name)\n",
    "        if self.model is None:\n",
    "            raise ValueError(f\"Model '{model_name}' not loaded\")\n",
    "        \n",
    "        # Cache token IDs for colors 0-9\n",
    "        self.color_tokens = self._get_color_tokens()\n",
    "        print(f\"  Expert '{expert_id}' using model: {model_name}\")\n",
    "    \n",
    "    def _get_color_tokens(self) -> Dict[int, int]:\n",
    "        \"\"\"Get token IDs for color digits 0-9.\"\"\"\n",
    "        tokens = {}\n",
    "        for color in range(10):\n",
    "            token_ids = self.tokenizer.encode(str(color), add_special_tokens=False)\n",
    "            if token_ids:\n",
    "                tokens[color] = token_ids[0]\n",
    "        return tokens\n",
    "    \n",
    "    def _format_grid(self, grid: np.ndarray) -> str:\n",
    "        \"\"\"Format grid for prompt - compact representation.\"\"\"\n",
    "        rows = []\n",
    "        for row in grid:\n",
    "            row_str = ''.join('.' if v == -1 else str(int(v)) for v in row)\n",
    "            rows.append(row_str)\n",
    "        return '\\n'.join(rows)\n",
    "    \n",
    "    def _format_grid_with_marker(self, grid: np.ndarray, target_row: int, target_col: int) -> str:\n",
    "        \"\"\"Format grid with a marker showing which cell to predict.\"\"\"\n",
    "        rows = []\n",
    "        for i, row in enumerate(grid):\n",
    "            row_chars = []\n",
    "            for j, v in enumerate(row):\n",
    "                if i == target_row and j == target_col:\n",
    "                    row_chars.append('?')  # Mark target cell\n",
    "                elif v == -1:\n",
    "                    row_chars.append('.')\n",
    "                else:\n",
    "                    row_chars.append(str(int(v)))\n",
    "            rows.append(''.join(row_chars))\n",
    "        return '\\n'.join(rows)\n",
    "    \n",
    "    def _create_cell_prompt(self, problem: Any, partial_grid: np.ndarray, \n",
    "                            row: int, col: int) -> str:\n",
    "        \"\"\"Create prompt for predicting a specific cell value.\"\"\"\n",
    "        prompt = \"\"\"You are solving an ARC (Abstraction and Reasoning Corpus) puzzle.\n",
    "\n",
    "Your task is to identify the TRANSFORMATION PATTERN from the training examples, then apply it to predict the output.\n",
    "\n",
    "Study how each input transforms to its output:\n",
    "- Look for patterns like: copying, mirroring, rotation, color replacement, counting, filling regions, etc.\n",
    "- The same transformation rule applies to all examples.\n",
    "\n",
    "Grid notation:\n",
    "- Numbers 0-9 represent colors\n",
    "- '.' represents unfilled cells (value -1) that need to be determined\n",
    "- '?' marks the specific cell you must predict\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        if hasattr(problem, 'train') and problem.train:\n",
    "            prompt += \"=== Training Examples (learn the pattern) ===\\n\"\n",
    "            for i, example in enumerate(problem.train[:3], 1):\n",
    "                inp = np.array(example.get('input', []))\n",
    "                out = np.array(example.get('output', []))\n",
    "                prompt += f\"\\nExample {i}:\\nInput:\\n{self._format_grid(inp)}\\n\"\n",
    "                prompt += f\"Output:\\n{self._format_grid(out)}\\n\"\n",
    "        \n",
    "        prompt += \"\\n=== Test Case ===\\n\"\n",
    "        prompt += \"Apply the same transformation pattern to fill in the output grid.\\n\"\n",
    "        prompt += f\"Current output grid (cells marked '.' are unfilled, '?' is your target):\\n\"\n",
    "        prompt += f\"{self._format_grid_with_marker(partial_grid, row, col)}\\n\\n\"\n",
    "        prompt += f\"Based on the transformation pattern and any already-filled cells, the value for '?' is: \"\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def _get_cell_logits_single(self, prompt: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get log-probabilities for colors 0-9 given a prompt.\n",
    "        \n",
    "        Returns:\n",
    "            Array of shape (10,) with log-probs for each color\n",
    "        \"\"\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs, use_cache=False)\n",
    "            logits = outputs.logits[0, -1, :]  # Last position\n",
    "        \n",
    "        # Extract logits for color tokens\n",
    "        color_logits = torch.zeros(10, device=logits.device)\n",
    "        for color, token_id in self.color_tokens.items():\n",
    "            color_logits[color] = logits[token_id]\n",
    "        \n",
    "        # Apply log_softmax\n",
    "        log_probs = F.log_softmax(color_logits, dim=0)\n",
    "        return log_probs.cpu().numpy()\n",
    "    \n",
    "    def get_cell_logits(self, problem: Any, partial_grid: np.ndarray) -> CellLogits:\n",
    "        \"\"\"\n",
    "        Get per-cell logits for the entire grid.\n",
    "        \n",
    "        Single forward pass per cell - no augmentation.\n",
    "        \"\"\"\n",
    "        h, w = partial_grid.shape\n",
    "        logits = np.zeros((h, w, 10))\n",
    "        \n",
    "        for i in range(h):\n",
    "            for j in range(w):\n",
    "                if partial_grid[i, j] >= 0:\n",
    "                    # Already filled - deterministic (100% mass on that color)\n",
    "                    logits[i, j, :] = -100.0\n",
    "                    logits[i, j, int(partial_grid[i, j])] = 0.0\n",
    "                else:\n",
    "                    # Query LLM for this cell\n",
    "                    prompt = self._create_cell_prompt(problem, partial_grid, i, j)\n",
    "                    logits[i, j] = self._get_cell_logits_single(prompt)\n",
    "        \n",
    "        return CellLogits(logits=logits)\n",
    "\n",
    "print(\"TransformerExpert defined with per-cell logits (no augmentation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load ARC-AGI Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARCTaskLoader is defined in the next cell along with data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to ARC data\n",
    "# Based on MARCO3 setup, data is at ../marco2/data/training/\n",
    "ARC_DATA_PATHS = [\n",
    "    '../marco2/data/training',      # MARCO3 location\n",
    "    '../marco2/data',               # Alternative\n",
    "    '/home/ubuntu/marco2/data/training',  # Absolute path on server\n",
    "    '/lambda/nfs/marco2/data/training',\n",
    "    '../arc-agi/data',\n",
    "    os.path.expanduser('~/arc-agi')\n",
    "]\n",
    "\n",
    "# Simple loader for flat directory of JSON files\n",
    "class SimpleARCLoader:\n",
    "    \"\"\"Load ARC tasks from a flat directory of JSON files.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path: str):\n",
    "        self.data_path = Path(data_path)\n",
    "        self.tasks = []\n",
    "        self._load_tasks()\n",
    "    \n",
    "    def _load_tasks(self):\n",
    "        \"\"\"Load all JSON task files.\"\"\"\n",
    "        json_files = list(self.data_path.glob('*.json'))\n",
    "        for json_file in sorted(json_files):\n",
    "            try:\n",
    "                with open(json_file, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                self.tasks.append({\n",
    "                    'task_id': json_file.stem,\n",
    "                    'train': data.get('train', []),\n",
    "                    'test': data.get('test', [])\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {json_file.name}: {e}\")\n",
    "        print(f\"Loaded {len(self.tasks)} tasks from {self.data_path}\")\n",
    "    \n",
    "    def get_task(self, task_id: str):\n",
    "        \"\"\"Get task by ID.\"\"\"\n",
    "        for task in self.tasks:\n",
    "            if task['task_id'] == task_id:\n",
    "                return task\n",
    "        return None\n",
    "    \n",
    "    def get_training_tasks(self):\n",
    "        return self.tasks\n",
    "    \n",
    "    def get_sample_tasks(self, n: int = 5, split: str = 'training'):\n",
    "        \"\"\"Get random sample of tasks.\"\"\"\n",
    "        indices = np.random.choice(len(self.tasks), size=min(n, len(self.tasks)), replace=False)\n",
    "        return [self.tasks[i] for i in indices]\n",
    "\n",
    "arc_loader = None\n",
    "for path in ARC_DATA_PATHS:\n",
    "    if os.path.exists(path):\n",
    "        print(f\"Found ARC data at: {path}\")\n",
    "        arc_loader = SimpleARCLoader(path)\n",
    "        break\n",
    "\n",
    "if arc_loader is None:\n",
    "    print(\"ARC data not found at expected locations.\")\n",
    "    print(\"Tried:\", ARC_DATA_PATHS)\n",
    "    print(\"\\nWill use synthetic demo tasks instead.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configure MARCO4 System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configuration\n",
    "config = MARCO4Config()\n",
    "\n",
    "# Pruning thresholds (MCU-level)\n",
    "config.pruning.mcu_prune_threshold = 0.05  # Prune branches below this combined mass\n",
    "config.pruning.max_conflict = 0.50         # Max conflict before pruning\n",
    "config.pruning.no_progress_rounds = 5      # Prune if no progress for N rounds\n",
    "\n",
    "# Confidence thresholds  \n",
    "config.confidence.solution_threshold = 0.80  # Accept solution above this\n",
    "config.confidence.high_confidence = 0.30     # Fix cell if belief > this\n",
    "\n",
    "# Search parameters\n",
    "config.search.max_iterations = 100\n",
    "config.search.max_branches = 100             # Max active branches in CSS\n",
    "config.search.beam_width = 10                # Top branches per iteration\n",
    "\n",
    "# MCU-driven branching\n",
    "config.search.branch_threshold = 0.15        # Create branches for candidates above this\n",
    "config.search.max_branches_per_cell = 3      # Max branches per uncertain cell\n",
    "\n",
    "# Expert parameters\n",
    "config.expert.num_experts = 3\n",
    "config.expert.temperature = 0.7\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(json.dumps(config.to_dict(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create MARCO4 System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one expert per loaded model - TRUE diversity\n",
    "# Each expert uses a DIFFERENT LLM (qwen, phi3, gpt-oss)\n",
    "experts = []\n",
    "for model_name in loaded_models:\n",
    "    expert = TransformerExpert(\n",
    "        expert_id=f\"expert_{model_name}\",\n",
    "        llm_manager=llm_manager,\n",
    "        model_name=model_name,\n",
    "        config=config\n",
    "    )\n",
    "    experts.append(expert)\n",
    "\n",
    "print(f\"\\nCreated {len(experts)} experts with different models:\")\n",
    "for exp in experts:\n",
    "    print(f\"  - {exp.expert_id} -> {exp.model_name}\")\n",
    "\n",
    "# Update config to match actual number of experts\n",
    "config.expert.num_experts = len(experts)\n",
    "\n",
    "# Create MCU with PARALLEL execution enabled\n",
    "mcu = MCU(experts, config, parallel=True)\n",
    "print(f\"\\nMCU initialized with {len(experts)} experts (parallel execution: ON)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# ARC color palette (colors 0-9)\n",
    "ARC_COLORS = [\n",
    "    '#000000',  # 0: black\n",
    "    '#0074D9',  # 1: blue\n",
    "    '#FF4136',  # 2: red\n",
    "    '#2ECC40',  # 3: green\n",
    "    '#FFDC00',  # 4: yellow\n",
    "    '#AAAAAA',  # 5: gray\n",
    "    '#F012BE',  # 6: magenta\n",
    "    '#FF851B',  # 7: orange\n",
    "    '#7FDBFF',  # 8: cyan\n",
    "    '#870C25',  # 9: brown\n",
    "]\n",
    "\n",
    "# Color for unfilled cells (-1)\n",
    "UNFILLED_COLOR = '#FFFFFF'  # White background\n",
    "UNFILLED_PATTERN_COLOR = '#CCCCCC'  # Light gray for pattern\n",
    "\n",
    "def plot_grid(grid: np.ndarray, title: str = \"\", ax=None, show_unfilled_count: bool = True):\n",
    "    \"\"\"\n",
    "    Plot a single grid with ARC colors.\n",
    "    \n",
    "    Unfilled cells (-1) are shown as white with a diagonal hatch pattern.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(4, 4))\n",
    "    \n",
    "    h, w = grid.shape\n",
    "    \n",
    "    # Create a masked array for proper color mapping\n",
    "    # We'll handle -1 cells separately\n",
    "    cmap = mcolors.ListedColormap(ARC_COLORS)\n",
    "    bounds = np.arange(-0.5, 10.5, 1)\n",
    "    norm = mcolors.BoundaryNorm(bounds, cmap.N)\n",
    "    \n",
    "    # Create display grid: replace -1 with 0 for imshow, we'll overlay unfilled cells\n",
    "    display_grid = grid.copy().astype(float)\n",
    "    unfilled_mask = (grid == -1)\n",
    "    display_grid[unfilled_mask] = np.nan  # Will show as white (bad color)\n",
    "    \n",
    "    # Set white background for the axes\n",
    "    ax.set_facecolor('white')\n",
    "    \n",
    "    # Plot the grid\n",
    "    ax.imshow(display_grid, cmap=cmap, norm=norm, interpolation='nearest')\n",
    "    \n",
    "    # Draw hatched rectangles for unfilled cells (-1)\n",
    "    unfilled_count = 0\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            if grid[i, j] == -1:\n",
    "                unfilled_count += 1\n",
    "                # Add a hatched rectangle to indicate unfilled\n",
    "                rect = Rectangle(\n",
    "                    (j - 0.5, i - 0.5), 1, 1,\n",
    "                    linewidth=0,\n",
    "                    edgecolor='none',\n",
    "                    facecolor=UNFILLED_COLOR,\n",
    "                    zorder=1\n",
    "                )\n",
    "                ax.add_patch(rect)\n",
    "                # Add diagonal lines pattern\n",
    "                ax.plot([j - 0.5, j + 0.5], [i - 0.5, i + 0.5], \n",
    "                       color=UNFILLED_PATTERN_COLOR, linewidth=1, zorder=2)\n",
    "                ax.plot([j - 0.5, j + 0.5], [i + 0.5, i - 0.5], \n",
    "                       color=UNFILLED_PATTERN_COLOR, linewidth=1, zorder=2)\n",
    "    \n",
    "    # Update title with unfilled count if requested\n",
    "    if show_unfilled_count and unfilled_count > 0:\n",
    "        total_cells = h * w\n",
    "        title = f\"{title}\\n[{unfilled_count}/{total_cells} unfilled]\"\n",
    "    \n",
    "    ax.set_title(title, fontsize=10)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlim(-0.5, w - 0.5)\n",
    "    ax.set_ylim(h - 0.5, -0.5)\n",
    "    \n",
    "    # Add grid lines\n",
    "    for i in range(h + 1):\n",
    "        ax.axhline(i - 0.5, color='#888888', linewidth=0.5, zorder=3)\n",
    "    for j in range(w + 1):\n",
    "        ax.axvline(j - 0.5, color='#888888', linewidth=0.5, zorder=3)\n",
    "\n",
    "def plot_task(task: Dict):\n",
    "    \"\"\"Plot all examples in a task.\"\"\"\n",
    "    train = task.get('train', [])\n",
    "    test = task.get('test', [])\n",
    "    \n",
    "    n_train = len(train)\n",
    "    n_test = len(test)\n",
    "    \n",
    "    fig, axes = plt.subplots(n_train + n_test, 2, figsize=(8, 4 * (n_train + n_test)))\n",
    "    if n_train + n_test == 1:\n",
    "        axes = axes.reshape(1, 2)\n",
    "    \n",
    "    for i, example in enumerate(train):\n",
    "        inp = np.array(example.get('input', example[0] if isinstance(example, tuple) else []))\n",
    "        out = np.array(example.get('output', example[1] if isinstance(example, tuple) else []))\n",
    "        plot_grid(inp, f\"Train {i+1} Input\", axes[i, 0], show_unfilled_count=False)\n",
    "        plot_grid(out, f\"Train {i+1} Output\", axes[i, 1], show_unfilled_count=False)\n",
    "    \n",
    "    for i, example in enumerate(test):\n",
    "        inp = np.array(example.get('input', example[0] if isinstance(example, tuple) else []))\n",
    "        plot_grid(inp, f\"Test {i+1} Input\", axes[n_train + i, 0], show_unfilled_count=False)\n",
    "        if 'output' in example:\n",
    "            out = np.array(example['output'])\n",
    "            plot_grid(out, f\"Test {i+1} Output (ground truth)\", axes[n_train + i, 1], show_unfilled_count=False)\n",
    "        else:\n",
    "            axes[n_train + i, 1].text(0.5, 0.5, \"?\", ha='center', va='center', fontsize=40)\n",
    "            axes[n_train + i, 1].set_title(\"Test Output (to predict)\")\n",
    "    \n",
    "    plt.suptitle(f\"Task: {task.get('task_id', 'Unknown')}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_solution_comparison(task: Dict, solution: np.ndarray):\n",
    "    \"\"\"Compare predicted solution with ground truth.\"\"\"\n",
    "    test = task.get('test', [{}])[0]\n",
    "    expected = np.array(test.get('output', [[]]))\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    \n",
    "    plot_grid(np.array(test.get('input', [[]])), \"Test Input\", axes[0], show_unfilled_count=False)\n",
    "    plot_grid(solution, \"Predicted\", axes[1])\n",
    "    \n",
    "    if expected.size > 0:\n",
    "        plot_grid(expected, \"Expected\", axes[2], show_unfilled_count=False)\n",
    "        correct = np.array_equal(solution, expected)\n",
    "        fig.suptitle(f\"{'✓ CORRECT' if correct else '✗ INCORRECT'}\", fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Visualization utilities loaded - unfilled cells (-1) shown with X pattern\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7b. Live Search Visualization\n",
    "\n",
    "The MCU now supports a progress callback that lets you visualize the grid evolution during search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "from marco.mcu import GridState\n",
    "\n",
    "class SearchVisualizer:\n",
    "    \"\"\"\n",
    "    Visualizes the evolution of the grid during MCU search.\n",
    "    \n",
    "    Shows:\n",
    "    - Filled cells with ARC colors\n",
    "    - Unfilled cells (-1) with X pattern\n",
    "    - Count of unfilled cells per iteration\n",
    "    \n",
    "    Usage:\n",
    "        visualizer = SearchVisualizer()\n",
    "        mcu_with_viz = MCU(experts, config, progress_callback=visualizer.callback)\n",
    "        result = mcu_with_viz.solve(problem, target_size=(h, w))\n",
    "        visualizer.show_history()  # Show all iterations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, live_update: bool = True, max_history: int = 50):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            live_update: If True, update display in real-time (requires Jupyter)\n",
    "            max_history: Maximum number of iterations to store\n",
    "        \"\"\"\n",
    "        self.live_update = live_update\n",
    "        self.max_history = max_history\n",
    "        self.history = []  # List of (iteration, grid, confidence, conflict, branches)\n",
    "        self.fig = None\n",
    "        self.axes = None\n",
    "    \n",
    "    def callback(self, iteration, grid_state, partial_grid, best_solution, \n",
    "                 best_confidence, conflict_level, active_branches, total_branches):\n",
    "        \"\"\"Progress callback for MCU.solve()\"\"\"\n",
    "        # Count unfilled cells\n",
    "        unfilled_cells = np.sum(partial_grid == -1)\n",
    "        total_cells = partial_grid.size\n",
    "        filled_cells = total_cells - unfilled_cells\n",
    "        \n",
    "        # Store in history\n",
    "        self.history.append({\n",
    "            'iteration': iteration,\n",
    "            'partial_grid': partial_grid.copy(),\n",
    "            'best_solution': best_solution.copy() if best_solution is not None else None,\n",
    "            'best_confidence': best_confidence,\n",
    "            'conflict_level': conflict_level,\n",
    "            'active_branches': active_branches,\n",
    "            'total_branches': total_branches,\n",
    "            'filled_cells': filled_cells,\n",
    "            'unfilled_cells': unfilled_cells,\n",
    "            'total_cells': total_cells\n",
    "        })\n",
    "        \n",
    "        # Trim history if needed\n",
    "        if len(self.history) > self.max_history:\n",
    "            self.history = self.history[-self.max_history:]\n",
    "        \n",
    "        # Live update\n",
    "        if self.live_update:\n",
    "            self._update_display()\n",
    "    \n",
    "    def _update_display(self):\n",
    "        \"\"\"Update the live display.\"\"\"\n",
    "        if not self.history:\n",
    "            return\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        latest = self.history[-1]\n",
    "        \n",
    "        # Create figure with current state\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "        \n",
    "        # Current partial grid - plot_grid now shows unfilled cells with X pattern\n",
    "        plot_grid(latest['partial_grid'], \n",
    "                  f\"Iteration {latest['iteration']}: Partial Grid\", axes[0])\n",
    "        \n",
    "        # Best solution so far (or partial if none)\n",
    "        if latest['best_solution'] is not None:\n",
    "            plot_grid(latest['best_solution'], \n",
    "                      f\"Best Solution (conf: {latest['best_confidence']:.3f})\", axes[1])\n",
    "        else:\n",
    "            axes[1].text(0.5, 0.5, \"No complete\\nsolution yet\", \n",
    "                        ha='center', va='center', fontsize=12)\n",
    "            axes[1].set_title(\"Best Solution\")\n",
    "            axes[1].set_facecolor('white')\n",
    "        \n",
    "        # Add stats as text - now includes unfilled count\n",
    "        stats_text = (\n",
    "            f\"Filled: {latest['filled_cells']}/{latest['total_cells']} | \"\n",
    "            f\"Unfilled: {latest['unfilled_cells']} | \"\n",
    "            f\"Conflict: {latest['conflict_level']:.3f} | \"\n",
    "            f\"Branches: {latest['active_branches']}/{latest['total_branches']}\"\n",
    "        )\n",
    "        fig.suptitle(stats_text, fontsize=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def show_history(self, step: int = 1):\n",
    "        \"\"\"\n",
    "        Show the evolution of the grid across iterations.\n",
    "        \n",
    "        Args:\n",
    "            step: Show every Nth iteration (default: 1 = all)\n",
    "        \"\"\"\n",
    "        if not self.history:\n",
    "            print(\"No history recorded.\")\n",
    "            return\n",
    "        \n",
    "        # Select iterations to show\n",
    "        selected = self.history[::step]\n",
    "        n = len(selected)\n",
    "        \n",
    "        # Create grid of plots\n",
    "        cols = min(4, n)\n",
    "        rows = (n + cols - 1) // cols\n",
    "        \n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(4 * cols, 4 * rows))\n",
    "        if rows == 1 and cols == 1:\n",
    "            axes = np.array([[axes]])\n",
    "        elif rows == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        elif cols == 1:\n",
    "            axes = axes.reshape(-1, 1)\n",
    "        \n",
    "        for idx, record in enumerate(selected):\n",
    "            row, col = idx // cols, idx % cols\n",
    "            ax = axes[row, col]\n",
    "            \n",
    "            grid = record['partial_grid']\n",
    "            # Title shows iteration and unfilled count\n",
    "            title = f\"Iter {record['iteration']}\"\n",
    "            plot_grid(grid, title, ax, show_unfilled_count=True)\n",
    "        \n",
    "        # Hide empty subplots\n",
    "        for idx in range(n, rows * cols):\n",
    "            row, col = idx // cols, idx % cols\n",
    "            axes[row, col].axis('off')\n",
    "        \n",
    "        plt.suptitle(\"Grid Evolution During Search (X = unfilled cells)\", fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def show_confidence_curve(self):\n",
    "        \"\"\"Plot confidence, fill progress, and unfilled cells over iterations.\"\"\"\n",
    "        if not self.history:\n",
    "            print(\"No history recorded.\")\n",
    "            return\n",
    "        \n",
    "        iterations = [h['iteration'] for h in self.history]\n",
    "        confidences = [h['best_confidence'] for h in self.history]\n",
    "        fill_pcts = [h['filled_cells'] / h['total_cells'] * 100 for h in self.history]\n",
    "        unfilled_counts = [h['unfilled_cells'] for h in self.history]\n",
    "        conflicts = [h['conflict_level'] for h in self.history]\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "        \n",
    "        # Confidence\n",
    "        axes[0, 0].plot(iterations, confidences, 'b-', linewidth=2, marker='o', markersize=4)\n",
    "        axes[0, 0].set_xlabel('Iteration')\n",
    "        axes[0, 0].set_ylabel('Best Confidence')\n",
    "        axes[0, 0].set_title('Confidence Evolution')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Fill percentage\n",
    "        axes[0, 1].plot(iterations, fill_pcts, 'g-', linewidth=2, marker='o', markersize=4)\n",
    "        axes[0, 1].set_xlabel('Iteration')\n",
    "        axes[0, 1].set_ylabel('Cells Filled (%)')\n",
    "        axes[0, 1].set_title('Grid Completion')\n",
    "        axes[0, 1].set_ylim(0, 105)\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Unfilled cells count\n",
    "        axes[1, 0].plot(iterations, unfilled_counts, 'm-', linewidth=2, marker='s', markersize=4)\n",
    "        axes[1, 0].set_xlabel('Iteration')\n",
    "        axes[1, 0].set_ylabel('Unfilled Cells (-1)')\n",
    "        axes[1, 0].set_title('Remaining Unfilled Cells')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        # Add horizontal line at 0\n",
    "        axes[1, 0].axhline(y=0, color='green', linestyle='--', alpha=0.5, label='Complete')\n",
    "        axes[1, 0].legend()\n",
    "        \n",
    "        # Conflict\n",
    "        axes[1, 1].plot(iterations, conflicts, 'r-', linewidth=2, marker='o', markersize=4)\n",
    "        axes[1, 1].set_xlabel('Iteration')\n",
    "        axes[1, 1].set_ylabel('Conflict Level')\n",
    "        axes[1, 1].set_title('Expert Conflict')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Clear history for a new search.\"\"\"\n",
    "        self.history = []\n",
    "\n",
    "print(\"SearchVisualizer defined - tracks unfilled cells (-1) with visual indicators!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Solve with live visualization\n",
    "# Set live_update=False if you just want to see the history after solving\n",
    "\n",
    "visualizer = SearchVisualizer(live_update=False)  # Set True for real-time updates\n",
    "\n",
    "# Create MCU with visualizer callback\n",
    "mcu_viz = MCU(experts, config, parallel=True, progress_callback=visualizer.callback)\n",
    "\n",
    "# Use a demo task for visualization\n",
    "viz_task = {\n",
    "    'task_id': 'viz_demo',\n",
    "    'train': [\n",
    "        {'input': [[0, 0], [0, 0]], 'output': [[1, 1], [1, 1]]},\n",
    "    ],\n",
    "    'test': [\n",
    "        {'input': [[0, 0, 0], [0, 0, 0]], 'output': [[1, 1, 1], [1, 1, 1]]}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Solve\n",
    "problem = ARCProblem(viz_task)\n",
    "result = mcu_viz.solve(problem, target_size=(2, 3))\n",
    "\n",
    "print(f\"Solved in {result.iterations} iterations\")\n",
    "print(f\"Status: {result.status}, Confidence: {result.confidence:.3f}\")\n",
    "\n",
    "# Show the grid evolution\n",
    "visualizer.show_history()\n",
    "\n",
    "# Show metrics curves\n",
    "visualizer.show_confidence_curve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Solve a Single Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_arc_task(task: Dict, mcu: MCU, verbose: bool = True, visualizer: SearchVisualizer = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Solve an ARC task using MARCO4.\n",
    "    \n",
    "    Args:\n",
    "        task: ARC task dict with train/test examples\n",
    "        mcu: MCU instance (will be recreated with visualizer if provided)\n",
    "        verbose: Print progress info\n",
    "        visualizer: Optional SearchVisualizer for tracking grid evolution\n",
    "    \n",
    "    Returns:\n",
    "        Dict with solution, confidence, metrics\n",
    "    \"\"\"\n",
    "    task_id = task.get('task_id', 'unknown')\n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Solving task: {task_id}\")\n",
    "        print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create ARCProblem\n",
    "    problem = ARCProblem(task)\n",
    "    \n",
    "    # Infer target size from training\n",
    "    target_size = None\n",
    "    if task.get('train'):\n",
    "        output = task['train'][0].get('output', [[]])\n",
    "        target_size = (len(output), len(output[0]) if output else 0)\n",
    "        if verbose:\n",
    "            print(f\"Target size: {target_size}\")\n",
    "    \n",
    "    # If visualizer provided, create MCU with callback\n",
    "    if visualizer is not None:\n",
    "        visualizer.reset()  # Clear previous history\n",
    "        solve_mcu = MCU(\n",
    "            mcu.experts, \n",
    "            mcu.config, \n",
    "            parallel=mcu.parallel,\n",
    "            progress_callback=visualizer.callback\n",
    "        )\n",
    "    else:\n",
    "        solve_mcu = mcu\n",
    "    \n",
    "    # Solve\n",
    "    start_time = time.time()\n",
    "    result = solve_mcu.solve(problem, target_size=target_size)\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nResult:\")\n",
    "        print(f\"  Status: {result.status}\")\n",
    "        print(f\"  Confidence: {result.confidence:.4f}\")\n",
    "        print(f\"  Iterations: {result.iterations}\")\n",
    "        print(f\"  Branches explored: {result.branches_explored}\")\n",
    "        print(f\"  Time: {elapsed:.2f}s\")\n",
    "    \n",
    "    # Evaluate if ground truth available\n",
    "    evaluation = None\n",
    "    if task.get('test') and 'output' in task['test'][0]:\n",
    "        expected = task['test'][0]['output']\n",
    "        if result.solution is not None:\n",
    "            evaluation = evaluate_solution(result.solution, expected)\n",
    "            if verbose:\n",
    "                print(f\"\\nEvaluation:\")\n",
    "                print(f\"  Correct: {evaluation['correct']}\")\n",
    "                print(f\"  Accuracy: {evaluation['accuracy']:.2%}\")\n",
    "    \n",
    "    return {\n",
    "        'task_id': task_id,\n",
    "        'solution': result.solution,\n",
    "        'confidence': result.confidence,\n",
    "        'status': result.status,\n",
    "        'iterations': result.iterations,\n",
    "        'branches': result.branches_explored,\n",
    "        'time': elapsed,\n",
    "        'evaluation': evaluation\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample task\n",
    "if arc_loader is not None:\n",
    "    sample_tasks = arc_loader.get_sample_tasks(n=1, split='training')\n",
    "    if sample_tasks:\n",
    "        task = sample_tasks[0]\n",
    "        print(f\"Selected task: {task['task_id']}\")\n",
    "        plot_task(task)\n",
    "else:\n",
    "    # Use a simple synthetic task for demo\n",
    "    task = {\n",
    "        'task_id': 'demo_fill_ones',\n",
    "        'train': [\n",
    "            {'input': [[0, 0], [0, 0]], 'output': [[1, 1], [1, 1]]},\n",
    "            {'input': [[0, 0, 0], [0, 0, 0]], 'output': [[1, 1, 1], [1, 1, 1]]}\n",
    "        ],\n",
    "        'test': [\n",
    "            {'input': [[0, 0], [0, 0], [0, 0]], 'output': [[1, 1], [1, 1], [1, 1]]}\n",
    "        ]\n",
    "    }\n",
    "    print(\"Using synthetic demo task\")\n",
    "    plot_task(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizer for tracking search progress\n",
    "# Set live_update=True to see real-time updates (may slow down solving)\n",
    "task_visualizer = SearchVisualizer(live_update=False)\n",
    "\n",
    "# Solve the task with visualization\n",
    "result = solve_arc_task(task, mcu, verbose=True, visualizer=task_visualizer)\n",
    "\n",
    "# Visualize solution comparison\n",
    "if result['solution'] is not None:\n",
    "    plot_solution_comparison(task, result['solution'])\n",
    "\n",
    "# Show grid evolution during search\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Grid Evolution During Search\")\n",
    "print(\"=\"*60)\n",
    "task_visualizer.show_history()\n",
    "\n",
    "# Show metrics curves\n",
    "task_visualizer.show_confidence_curve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Batch Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_batch(tasks: List[Dict], mcu: MCU, verbose: bool = False) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate MARCO4 on a batch of tasks.\n",
    "    \n",
    "    Returns:\n",
    "        Summary statistics\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for i, task in enumerate(tasks):\n",
    "        print(f\"\\nTask {i+1}/{len(tasks)}: {task.get('task_id', 'unknown')}\")\n",
    "        \n",
    "        try:\n",
    "            result = solve_arc_task(task, mcu, verbose=verbose)\n",
    "            results.append(result)\n",
    "            \n",
    "            if result['evaluation'] is not None:\n",
    "                total += 1\n",
    "                if result['evaluation']['correct']:\n",
    "                    correct += 1\n",
    "                    print(f\"  ✓ Correct (confidence: {result['confidence']:.3f})\")\n",
    "                else:\n",
    "                    print(f\"  ✗ Incorrect (accuracy: {result['evaluation']['accuracy']:.1%})\")\n",
    "            else:\n",
    "                print(f\"  ? No ground truth (confidence: {result['confidence']:.3f})\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {e}\")\n",
    "            results.append({'task_id': task.get('task_id'), 'error': str(e)})\n",
    "    \n",
    "    # Summary\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    avg_time = np.mean([r.get('time', 0) for r in results if 'time' in r])\n",
    "    avg_iterations = np.mean([r.get('iterations', 0) for r in results if 'iterations' in r])\n",
    "    \n",
    "    summary = {\n",
    "        'total_tasks': len(tasks),\n",
    "        'evaluated': total,\n",
    "        'correct': correct,\n",
    "        'accuracy': accuracy,\n",
    "        'avg_time': avg_time,\n",
    "        'avg_iterations': avg_iterations,\n",
    "        'results': results\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"BATCH RESULTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Accuracy: {correct}/{total} = {accuracy:.1%}\")\n",
    "    print(f\"Avg time: {avg_time:.2f}s\")\n",
    "    print(f\"Avg iterations: {avg_iterations:.1f}\")\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on sample of tasks\n",
    "if arc_loader is not None:\n",
    "    sample_tasks = arc_loader.get_sample_tasks(n=5, split='training')\n",
    "    summary = evaluate_batch(sample_tasks, mcu, verbose=False)\n",
    "else:\n",
    "    print(\"No ARC data loaded. Skipping batch evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Analyze D-S Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate Dempster-Shafer combination\n",
    "from marco.dempster_shafer import (\n",
    "    dempster_combine, compute_belief, compute_plausibility,\n",
    "    get_pignistic_distribution, get_conflict_level, mass_to_string, THETA\n",
    ")\n",
    "\n",
    "print(\"Dempster-Shafer Combination Demo\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Expert 1: High confidence in color 1\n",
    "m1 = {frozenset([1]): 0.8, THETA: 0.2}\n",
    "print(f\"\\nExpert 1: {mass_to_string(m1)}\")\n",
    "\n",
    "# Expert 2: Medium confidence in color 1\n",
    "m2 = {frozenset([1]): 0.6, frozenset([1, 2]): 0.2, THETA: 0.2}\n",
    "print(f\"Expert 2: {mass_to_string(m2)}\")\n",
    "\n",
    "# Combine\n",
    "combined = dempster_combine(m1, m2)\n",
    "print(f\"\\nCombined: {mass_to_string(combined)}\")\n",
    "\n",
    "# Pignistic probabilities\n",
    "pignistic = get_pignistic_distribution(combined)\n",
    "print(f\"\\nPignistic probabilities:\")\n",
    "for color, prob in sorted(pignistic.items(), key=lambda x: -x[1]):\n",
    "    if prob > 0.01:\n",
    "        print(f\"  Color {color}: {prob:.3f}\")\n",
    "\n",
    "# Conflict example\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"High Conflict Example\")\n",
    "m3 = {frozenset([1]): 0.9, THETA: 0.1}\n",
    "m4 = {frozenset([2]): 0.9, THETA: 0.1}\n",
    "print(f\"\\nExpert A: {mass_to_string(m3)}\")\n",
    "print(f\"Expert B: {mass_to_string(m4)}\")\n",
    "\n",
    "conflict = get_conflict_level(m3, m4)\n",
    "print(f\"\\nConflict level K = {conflict:.3f}\")\n",
    "\n",
    "combined_conflict = dempster_combine(m3, m4)\n",
    "print(f\"Combined (with high conflict): {mass_to_string(combined_conflict)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. CSS Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze CSS after solving\n",
    "stats = mcu.get_statistics()\n",
    "\n",
    "print(\"Cognitive State Space Statistics\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total branches created: {stats['css']['total_created']}\")\n",
    "print(f\"Active branches: {stats['css']['active_branches']}\")\n",
    "print(f\"Complete branches: {stats['css']['complete_branches']}\")\n",
    "print(f\"Pruned branches: {stats['css']['pruned_branches']}\")\n",
    "print(f\"\\nMax active mass: {stats['css']['max_active_mass']:.4f}\")\n",
    "print(f\"Avg active mass: {stats['css']['avg_active_mass']:.4f}\")\n",
    "print(f\"Max complete mass: {stats['css']['max_complete_mass']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unload model to free memory\n",
    "llm_manager.unload()\n",
    "\n",
    "# Clear CUDA cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"GPU memory freed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
