{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Expert-Only Output Viewer\n",
    "\n",
    "This notebook shows the raw output from each LLM expert **without** Dempster-Shafer combination or MCU orchestration.\n",
    "\n",
    "For each expert, you can see:\n",
    "- Per-cell probability distributions (colors 0-9)\n",
    "- The predicted grid based on argmax of probabilities\n",
    "- Confidence levels for each cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import time\n",
    "\n",
    "# Add MARCO4 to path\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "# Basic imports from MARCO4 (no MCU/D-S needed)\n",
    "from marco.utils import create_empty_grid, grid_to_string\n",
    "from marco.main import ARCProblem, evaluate_solution\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Load LLM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, BitsAndBytesConfig\n",
    "import gc\n",
    "\n",
    "# Models that are already quantized (don't apply additional quantization)\n",
    "PREQUANTIZED_MODELS = {'gpt-oss'}\n",
    "BFLOAT16_MODELS = {'gpt-oss'}\n",
    "\n",
    "class LLMManager:\n",
    "    \"\"\"Manages multiple LLM models for multi-expert system.\"\"\"\n",
    "    \n",
    "    def __init__(self, device: str = \"auto\", use_4bit: bool = True):\n",
    "        self.device = device if device != \"auto\" else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.use_4bit = use_4bit and self.device == \"cuda\"\n",
    "        self.models = {}\n",
    "        self.tokenizers = {}\n",
    "        print(f\"LLMManager initialized with device: {self.device}\")\n",
    "    \n",
    "    def load_model(self, model_path: str, model_name: str = None):\n",
    "        \"\"\"Load a model from path.\"\"\"\n",
    "        model_name = model_name or Path(model_path).name\n",
    "        print(f\"\\nLoading {model_name} from: {model_path}\")\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        is_prequantized = model_name in PREQUANTIZED_MODELS\n",
    "        requires_bfloat16 = model_name in BFLOAT16_MODELS\n",
    "        \n",
    "        if self.device == \"cuda\":\n",
    "            torch_dtype = torch.bfloat16 if requires_bfloat16 else torch.float16\n",
    "        else:\n",
    "            torch_dtype = torch.float32\n",
    "        \n",
    "        load_kwargs = {\n",
    "            'device_map': \"auto\" if self.device == \"cuda\" else None,\n",
    "            'torch_dtype': torch_dtype,\n",
    "            'trust_remote_code': True,\n",
    "            'low_cpu_mem_usage': True,\n",
    "        }\n",
    "        \n",
    "        if self.use_4bit and not is_prequantized:\n",
    "            load_kwargs['quantization_config'] = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\"\n",
    "            )\n",
    "            print(\"  Using 4-bit quantization (BitsAndBytes)\")\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(model_path, **load_kwargs)\n",
    "        model.eval()\n",
    "        \n",
    "        self.models[model_name] = model\n",
    "        self.tokenizers[model_name] = tokenizer\n",
    "        \n",
    "        params = sum(p.numel() for p in model.parameters()) / 1e9\n",
    "        print(f\"  ✓ {model_name} loaded: {params:.2f}B parameters\")\n",
    "        \n",
    "        return model_name\n",
    "    \n",
    "    def get_model(self, model_name: str):\n",
    "        return self.models.get(model_name), self.tokenizers.get(model_name)\n",
    "    \n",
    "    def list_models(self):\n",
    "        return list(self.models.keys())\n",
    "    \n",
    "    def unload(self, model_name: str = None):\n",
    "        if model_name:\n",
    "            if model_name in self.models:\n",
    "                del self.models[model_name]\n",
    "                del self.tokenizers[model_name]\n",
    "        else:\n",
    "            self.models.clear()\n",
    "            self.tokenizers.clear()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "llm_manager = LLMManager(device=\"auto\", use_4bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model paths\n",
    "MODEL_PATHS = {\n",
    "    'qwen': '../models/qwen',\n",
    "    'phi3': '../models/phi3',\n",
    "    'gpt-oss': '../models/gpt-oss'\n",
    "}\n",
    "\n",
    "# Load ALL models\n",
    "loaded_models = []\n",
    "for name, path in MODEL_PATHS.items():\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            llm_manager.load_model(path, name)\n",
    "            loaded_models.append(name)\n",
    "        except Exception as e:\n",
    "            print(f\"  Failed to load {name}: {e}\")\n",
    "    else:\n",
    "        print(f\"  Model path not found: {path}\")\n",
    "\n",
    "print(f\"\\nLoaded {len(loaded_models)} models: {loaded_models}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. Define Expert (Standalone, No MARCO4 Dependencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": "import torch.nn.functional as F\nimport re\n\nclass StandaloneExpert:\n    \"\"\"\n    Standalone expert that predicts ARC output grids.\n    \n    Supports three modes:\n    - 'direct': Per-cell next-token probabilities (fast but limited)\n    - 'cell_reasoning': Per-cell with chain-of-thought (slower, more accurate)\n    - 'whole_grid': Predict entire grid at once (recommended)\n    \"\"\"\n    \n    def __init__(self, expert_id: str, llm_manager: LLMManager, model_name: str, \n                 mode: str = 'whole_grid', max_tokens: int = 500):\n        \"\"\"\n        Args:\n            mode: 'direct', 'cell_reasoning', or 'whole_grid'\n            max_tokens: Max tokens to generate\n        \"\"\"\n        self.expert_id = expert_id\n        self.model_name = model_name\n        self.mode = mode\n        self.max_tokens = max_tokens\n        self.model, self.tokenizer = llm_manager.get_model(model_name)\n        if self.model is None:\n            raise ValueError(f\"Model '{model_name}' not loaded\")\n        \n        self.color_tokens = self._get_color_tokens()\n        print(f\"Expert '{expert_id}' using model: {model_name} (mode: {mode})\")\n    \n    def _get_color_tokens(self) -> Dict[int, int]:\n        \"\"\"Get token IDs for color digits 0-9.\"\"\"\n        tokens = {}\n        for color in range(10):\n            token_ids = self.tokenizer.encode(str(color), add_special_tokens=False)\n            if token_ids:\n                tokens[color] = token_ids[0]\n        return tokens\n    \n    def _format_grid(self, grid: np.ndarray) -> str:\n        \"\"\"Format grid for prompt.\"\"\"\n        rows = []\n        for row in grid:\n            row_str = ''.join('.' if v == -1 else str(int(v)) for v in row)\n            rows.append(row_str)\n        return '\\n'.join(rows)\n    \n    def _format_grid_with_marker(self, grid: np.ndarray, target_row: int, target_col: int) -> str:\n        \"\"\"Format grid with a marker showing which cell to predict.\"\"\"\n        rows = []\n        for i, row in enumerate(grid):\n            row_chars = []\n            for j, v in enumerate(row):\n                if i == target_row and j == target_col:\n                    row_chars.append('?')\n                elif v == -1:\n                    row_chars.append('.')\n                else:\n                    row_chars.append(str(int(v)))\n            rows.append(''.join(row_chars))\n        return '\\n'.join(rows)\n    \n    def _create_whole_grid_prompt(self, task: Dict, target_size: Tuple[int, int]) -> str:\n        \"\"\"Create prompt for predicting the entire output grid at once.\"\"\"\n        h, w = target_size\n        \n        prompt = \"\"\"You are solving an ARC (Abstraction and Reasoning Corpus) puzzle.\n\nStudy the training examples to find the transformation pattern from input to output.\nThen apply the SAME pattern to the test input to produce the output.\n\nGrid notation: Numbers 0-9 represent colors.\n\n\"\"\"\n        \n        train = task.get('train', [])\n        if train:\n            prompt += \"=== Training Examples ===\\n\"\n            for i, example in enumerate(train, 1):\n                inp = np.array(example.get('input', []))\n                out = np.array(example.get('output', []))\n                prompt += f\"\\nExample {i}:\\nInput:\\n{self._format_grid(inp)}\\nOutput:\\n{self._format_grid(out)}\\n\"\n        \n        # Test input\n        test_input = np.array(task['test'][0]['input'])\n        prompt += f\"\"\"\n=== Test ===\nInput:\n{self._format_grid(test_input)}\n\nNow apply the same transformation pattern. Output the {h}x{w} grid with one row per line, using only digits 0-9:\nOutput:\n\"\"\"\n        \n        return prompt\n    \n    def _create_cell_prompt(self, task: Dict, partial_grid: np.ndarray, \n                            row: int, col: int, with_reasoning: bool = False) -> str:\n        \"\"\"Create prompt for predicting a specific cell.\"\"\"\n        if with_reasoning:\n            prompt = \"\"\"You are solving an ARC puzzle. Think step-by-step.\n\nGrid notation: 0-9 = colors, '.' = unfilled, '?' = cell to predict\n\n\"\"\"\n        else:\n            prompt = \"\"\"You are solving an ARC puzzle.\n\nGrid notation: 0-9 = colors, '.' = unfilled, '?' = cell to predict\n\n\"\"\"\n        \n        train = task.get('train', [])\n        if train:\n            prompt += \"=== Training Examples ===\\n\"\n            for i, example in enumerate(train[:3], 1):\n                inp = np.array(example.get('input', []))\n                out = np.array(example.get('output', []))\n                prompt += f\"\\nExample {i}:\\nInput:\\n{self._format_grid(inp)}\\nOutput:\\n{self._format_grid(out)}\\n\"\n        \n        prompt += f\"\\n=== Test ===\\nCurrent output (predict '?'):\\n\"\n        prompt += f\"{self._format_grid_with_marker(partial_grid, row, col)}\\n\\n\"\n        \n        if with_reasoning:\n            prompt += f\"Cell ({row},{col}): Let me analyze the pattern. \"\n        else:\n            prompt += f\"The value for '?' is: \"\n        \n        return prompt\n    \n    def _generate_text(self, prompt: str, max_tokens: int = None) -> str:\n        \"\"\"Generate text from prompt.\"\"\"\n        if max_tokens is None:\n            max_tokens = self.max_tokens\n            \n        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n        \n        with torch.no_grad():\n            outputs = self.model.generate(\n                **inputs,\n                max_new_tokens=max_tokens,\n                do_sample=False,\n                temperature=None,\n                top_p=None,\n                pad_token_id=self.tokenizer.pad_token_id,\n                eos_token_id=self.tokenizer.eos_token_id,\n            )\n        \n        generated = outputs[0][inputs['input_ids'].shape[1]:]\n        return self.tokenizer.decode(generated, skip_special_tokens=True)\n    \n    def _get_next_token_probs(self, prompt: str) -> np.ndarray:\n        \"\"\"Get immediate next-token probabilities for colors 0-9.\"\"\"\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n        \n        with torch.no_grad():\n            outputs = self.model(**inputs, use_cache=False)\n            logits = outputs.logits[0, -1, :]\n        \n        color_logits = torch.zeros(10, device=logits.device)\n        for color, token_id in self.color_tokens.items():\n            color_logits[color] = logits[token_id]\n        \n        probs = F.softmax(color_logits, dim=0)\n        return probs.cpu().numpy()\n    \n    def _parse_grid_from_text(self, text: str, target_size: Tuple[int, int]) -> Optional[np.ndarray]:\n        \"\"\"Parse a grid from generated text.\"\"\"\n        h, w = target_size\n        \n        # Find lines that look like grid rows (only digits, length matches width)\n        lines = text.strip().split('\\n')\n        grid_lines = []\n        \n        for line in lines:\n            # Clean the line - keep only digits\n            cleaned = ''.join(c for c in line if c.isdigit())\n            if len(cleaned) == w:\n                grid_lines.append(cleaned)\n            elif len(cleaned) > w:\n                # Take first w digits\n                grid_lines.append(cleaned[:w])\n        \n        if len(grid_lines) >= h:\n            # Take first h rows\n            grid = np.zeros((h, w), dtype=int)\n            for i in range(h):\n                for j in range(w):\n                    grid[i, j] = int(grid_lines[i][j])\n            return grid\n        \n        return None\n    \n    def _extract_digit(self, text: str) -> Optional[int]:\n        \"\"\"Extract a single digit answer from text.\"\"\"\n        text = text.lower()\n        \n        # Look for explicit patterns\n        patterns = [\n            r\"the answer is[:\\s]*(\\d)\",\n            r\"the value is[:\\s]*(\\d)\",\n            r\"should be[:\\s]*(\\d)\",\n            r\"is[:\\s]*(\\d)\\b\",\n            r\"= ?(\\d)\",\n            r\"\\*\\*(\\d)\\*\\*\",\n        ]\n        \n        for pattern in patterns:\n            match = re.search(pattern, text)\n            if match:\n                return int(match.group(1))\n        \n        # Fallback: first digit found\n        digits = re.findall(r'\\b(\\d)\\b', text)\n        if digits:\n            return int(digits[0])\n        \n        return None\n    \n    def predict_grid(self, task: Dict, target_size: Tuple[int, int], \n                     verbose: bool = False) -> Dict:\n        \"\"\"\n        Predict the output grid.\n        \n        Returns:\n            Dict with 'grid', 'probabilities', 'confidences', and optionally 'raw_output'/'reasoning'\n        \"\"\"\n        h, w = target_size\n        \n        if self.mode == 'whole_grid':\n            return self._predict_whole_grid(task, target_size, verbose)\n        elif self.mode == 'cell_reasoning':\n            return self._predict_cell_by_cell(task, target_size, with_reasoning=True, verbose=verbose)\n        else:  # direct\n            return self._predict_cell_by_cell(task, target_size, with_reasoning=False, verbose=verbose)\n    \n    def _predict_whole_grid(self, task: Dict, target_size: Tuple[int, int], \n                            verbose: bool = False) -> Dict:\n        \"\"\"Predict entire grid at once.\"\"\"\n        h, w = target_size\n        \n        print(f\"  Generating complete {h}x{w} grid...\")\n        \n        prompt = self._create_whole_grid_prompt(task, target_size)\n        raw_output = self._generate_text(prompt)\n        \n        if verbose:\n            print(f\"  Raw output:\\n{raw_output[:500]}\")\n        \n        # Parse grid from output\n        grid = self._parse_grid_from_text(raw_output, target_size)\n        \n        if grid is None:\n            print(f\"  WARNING: Could not parse grid from output, using zeros\")\n            grid = np.zeros((h, w), dtype=int)\n        \n        # For whole_grid mode, we don't have per-cell probabilities\n        # Use uniform probabilities with slight boost for predicted value\n        probabilities = np.ones((h, w, 10)) * 0.05\n        for i in range(h):\n            for j in range(w):\n                probabilities[i, j, grid[i, j]] = 0.55\n        probabilities = probabilities / probabilities.sum(axis=2, keepdims=True)\n        \n        confidences = np.max(probabilities, axis=2)\n        \n        return {\n            'grid': grid,\n            'probabilities': probabilities,\n            'confidences': confidences,\n            'raw_output': raw_output\n        }\n    \n    def _predict_cell_by_cell(self, task: Dict, target_size: Tuple[int, int],\n                              with_reasoning: bool = False, verbose: bool = False) -> Dict:\n        \"\"\"Predict grid cell-by-cell.\"\"\"\n        h, w = target_size\n        partial_grid = create_empty_grid(h, w)\n        \n        probabilities = np.zeros((h, w, 10))\n        predicted_grid = np.zeros((h, w), dtype=int)\n        confidences = np.zeros((h, w))\n        reasoning_texts = []\n        \n        mode_name = \"cell_reasoning\" if with_reasoning else \"direct\"\n        print(f\"  Predicting {h}x{w} = {h*w} cells ({mode_name})...\")\n        \n        for i in range(h):\n            for j in range(w):\n                prompt = self._create_cell_prompt(task, partial_grid, i, j, with_reasoning)\n                \n                if with_reasoning:\n                    # Generate reasoning and extract answer\n                    reasoning = self._generate_text(prompt, max_tokens=200)\n                    answer = self._extract_digit(reasoning)\n                    reasoning_texts.append({'cell': (i, j), 'text': reasoning})\n                    \n                    if verbose:\n                        print(f\"    ({i},{j}): {reasoning[:80]}... -> {answer}\")\n                    \n                    # Create probability distribution\n                    probs = np.ones(10) * 0.02\n                    if answer is not None and 0 <= answer <= 9:\n                        probs[answer] = 0.82\n                    probs = probs / probs.sum()\n                else:\n                    # Direct next-token probabilities\n                    probs = self._get_next_token_probs(prompt)\n                \n                probabilities[i, j] = probs\n                predicted_grid[i, j] = np.argmax(probs)\n                confidences[i, j] = np.max(probs)\n                \n                partial_grid[i, j] = predicted_grid[i, j]\n        \n        result = {\n            'grid': predicted_grid,\n            'probabilities': probabilities,\n            'confidences': confidences,\n        }\n        \n        if with_reasoning:\n            result['reasoning'] = reasoning_texts\n        \n        return result\n\nprint(\"StandaloneExpert defined with modes: 'direct', 'cell_reasoning', 'whole_grid'\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 4. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# ARC color palette\n",
    "ARC_COLORS = [\n",
    "    '#000000',  # 0: black\n",
    "    '#0074D9',  # 1: blue\n",
    "    '#FF4136',  # 2: red\n",
    "    '#2ECC40',  # 3: green\n",
    "    '#FFDC00',  # 4: yellow\n",
    "    '#AAAAAA',  # 5: gray\n",
    "    '#F012BE',  # 6: magenta\n",
    "    '#FF851B',  # 7: orange\n",
    "    '#7FDBFF',  # 8: cyan\n",
    "    '#870C25',  # 9: brown\n",
    "]\n",
    "\n",
    "def plot_grid(grid: np.ndarray, title: str = \"\", ax=None, show_values: bool = False):\n",
    "    \"\"\"Plot a grid with ARC colors.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(4, 4))\n",
    "    \n",
    "    h, w = grid.shape\n",
    "    cmap = mcolors.ListedColormap(ARC_COLORS)\n",
    "    bounds = np.arange(-0.5, 10.5, 1)\n",
    "    norm = mcolors.BoundaryNorm(bounds, cmap.N)\n",
    "    \n",
    "    # Handle -1 cells\n",
    "    display_grid = grid.copy().astype(float)\n",
    "    display_grid[grid == -1] = np.nan\n",
    "    ax.set_facecolor('white')\n",
    "    \n",
    "    ax.imshow(display_grid, cmap=cmap, norm=norm, interpolation='nearest')\n",
    "    \n",
    "    # Draw X for unfilled cells\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            if grid[i, j] == -1:\n",
    "                rect = Rectangle((j - 0.5, i - 0.5), 1, 1, facecolor='white', zorder=1)\n",
    "                ax.add_patch(rect)\n",
    "                ax.plot([j - 0.5, j + 0.5], [i - 0.5, i + 0.5], color='#CCCCCC', linewidth=1, zorder=2)\n",
    "                ax.plot([j - 0.5, j + 0.5], [i + 0.5, i - 0.5], color='#CCCCCC', linewidth=1, zorder=2)\n",
    "            elif show_values:\n",
    "                # Show cell value as text\n",
    "                text_color = 'white' if grid[i, j] in [0, 9] else 'black'\n",
    "                ax.text(j, i, str(int(grid[i, j])), ha='center', va='center', \n",
    "                       fontsize=10, color=text_color, fontweight='bold')\n",
    "    \n",
    "    ax.set_title(title, fontsize=10)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlim(-0.5, w - 0.5)\n",
    "    ax.set_ylim(h - 0.5, -0.5)\n",
    "    \n",
    "    for i in range(h + 1):\n",
    "        ax.axhline(i - 0.5, color='#888888', linewidth=0.5, zorder=3)\n",
    "    for j in range(w + 1):\n",
    "        ax.axvline(j - 0.5, color='#888888', linewidth=0.5, zorder=3)\n",
    "\n",
    "def plot_confidence_heatmap(confidences: np.ndarray, title: str = \"\", ax=None):\n",
    "    \"\"\"Plot confidence values as a heatmap.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(4, 4))\n",
    "    \n",
    "    h, w = confidences.shape\n",
    "    im = ax.imshow(confidences, cmap='RdYlGn', vmin=0, vmax=1, interpolation='nearest')\n",
    "    \n",
    "    # Show values\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            val = confidences[i, j]\n",
    "            text_color = 'white' if val < 0.5 else 'black'\n",
    "            ax.text(j, i, f'{val:.2f}', ha='center', va='center', \n",
    "                   fontsize=8, color=text_color)\n",
    "    \n",
    "    ax.set_title(title, fontsize=10)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    for i in range(h + 1):\n",
    "        ax.axhline(i - 0.5, color='#888888', linewidth=0.5)\n",
    "    for j in range(w + 1):\n",
    "        ax.axvline(j - 0.5, color='#888888', linewidth=0.5)\n",
    "    \n",
    "    return im\n",
    "\n",
    "def plot_probability_distribution(probs: np.ndarray, row: int, col: int, ax=None):\n",
    "    \"\"\"Plot probability distribution for a single cell.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(6, 3))\n",
    "    \n",
    "    colors = ARC_COLORS\n",
    "    bars = ax.bar(range(10), probs, color=colors, edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    ax.set_xlabel('Color')\n",
    "    ax.set_ylabel('Probability')\n",
    "    ax.set_title(f'Cell ({row}, {col}) Probability Distribution')\n",
    "    ax.set_xticks(range(10))\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Annotate max\n",
    "    max_idx = np.argmax(probs)\n",
    "    ax.annotate(f'{probs[max_idx]:.2f}', xy=(max_idx, probs[max_idx]), \n",
    "               ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "print(\"Visualization utilities loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 5. Load ARC Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARC_DATA_PATHS = [\n",
    "    '../marco2/data/training',\n",
    "    '../marco2/data',\n",
    "    '/home/ubuntu/marco2/data/training',\n",
    "    '/lambda/nfs/marco2/data/training',\n",
    "]\n",
    "\n",
    "class SimpleARCLoader:\n",
    "    def __init__(self, data_path: str):\n",
    "        self.data_path = Path(data_path)\n",
    "        self.tasks = []\n",
    "        self._load_tasks()\n",
    "    \n",
    "    def _load_tasks(self):\n",
    "        json_files = list(self.data_path.glob('*.json'))\n",
    "        for json_file in sorted(json_files):\n",
    "            try:\n",
    "                with open(json_file, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                self.tasks.append({\n",
    "                    'task_id': json_file.stem,\n",
    "                    'train': data.get('train', []),\n",
    "                    'test': data.get('test', [])\n",
    "                })\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        print(f\"Loaded {len(self.tasks)} tasks from {self.data_path}\")\n",
    "    \n",
    "    def get_task(self, task_id: str):\n",
    "        for task in self.tasks:\n",
    "            if task['task_id'] == task_id:\n",
    "                return task\n",
    "        return None\n",
    "    \n",
    "    def get_sample_tasks(self, n: int = 5):\n",
    "        indices = np.random.choice(len(self.tasks), size=min(n, len(self.tasks)), replace=False)\n",
    "        return [self.tasks[i] for i in indices]\n",
    "\n",
    "arc_loader = None\n",
    "for path in ARC_DATA_PATHS:\n",
    "    if os.path.exists(path):\n",
    "        print(f\"Found ARC data at: {path}\")\n",
    "        arc_loader = SimpleARCLoader(path)\n",
    "        break\n",
    "\n",
    "if arc_loader is None:\n",
    "    print(\"ARC data not found. Using demo task.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 6. Create Experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": "# Create one expert per loaded model\n# \n# Choose mode:\n#   'whole_grid'     - Predict entire grid at once (RECOMMENDED - most natural for LLMs)\n#   'cell_reasoning' - Per-cell with chain-of-thought reasoning\n#   'direct'         - Per-cell immediate next-token (fast but inaccurate)\n#\nEXPERT_MODE = 'whole_grid'\nMAX_TOKENS = 500  # Tokens to generate (increase for larger grids)\n\nexperts = {}\nfor model_name in loaded_models:\n    expert = StandaloneExpert(\n        expert_id=f\"expert_{model_name}\",\n        llm_manager=llm_manager,\n        model_name=model_name,\n        mode=EXPERT_MODE,\n        max_tokens=MAX_TOKENS\n    )\n    experts[model_name] = expert\n\nprint(f\"\\nCreated {len(experts)} experts: {list(experts.keys())}\")\nprint(f\"Mode: {EXPERT_MODE}\")\nprint(f\"Max tokens: {MAX_TOKENS}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 7. Select a Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample task or use demo\n",
    "if arc_loader is not None:\n",
    "    sample_tasks = arc_loader.get_sample_tasks(n=1)\n",
    "    task = sample_tasks[0] if sample_tasks else None\n",
    "else:\n",
    "    task = None\n",
    "\n",
    "if task is None:\n",
    "    # Demo task: fill with 1s\n",
    "    task = {\n",
    "        'task_id': 'demo_fill_ones',\n",
    "        'train': [\n",
    "            {'input': [[0, 0], [0, 0]], 'output': [[1, 1], [1, 1]]},\n",
    "            {'input': [[0, 0, 0], [0, 0, 0]], 'output': [[1, 1, 1], [1, 1, 1]]}\n",
    "        ],\n",
    "        'test': [\n",
    "            {'input': [[0, 0], [0, 0], [0, 0]], 'output': [[1, 1], [1, 1], [1, 1]]}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "print(f\"Task: {task['task_id']}\")\n",
    "print(f\"Training examples: {len(task['train'])}\")\n",
    "print(f\"Test examples: {len(task['test'])}\")\n",
    "\n",
    "# Determine target size\n",
    "train_output = task['train'][0]['output']\n",
    "target_size = (len(train_output), len(train_output[0]))\n",
    "print(f\"Target size: {target_size}\")\n",
    "\n",
    "# Show task\n",
    "n_train = len(task['train'])\n",
    "fig, axes = plt.subplots(n_train, 2, figsize=(8, 4 * n_train))\n",
    "if n_train == 1:\n",
    "    axes = axes.reshape(1, 2)\n",
    "\n",
    "for i, example in enumerate(task['train']):\n",
    "    inp = np.array(example['input'])\n",
    "    out = np.array(example['output'])\n",
    "    plot_grid(inp, f\"Train {i+1} Input\", axes[i, 0])\n",
    "    plot_grid(out, f\"Train {i+1} Output\", axes[i, 1])\n",
    "\n",
    "plt.suptitle(f\"Task: {task['task_id']}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 8. Get Predictions from Each Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions from each expert\n",
    "expert_results = {}\n",
    "\n",
    "for model_name, expert in experts.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Expert: {model_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    result = expert.predict_grid(task, target_size)\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    expert_results[model_name] = result\n",
    "    \n",
    "    print(f\"  Time: {elapsed:.2f}s\")\n",
    "    print(f\"  Avg confidence: {result['confidences'].mean():.3f}\")\n",
    "    print(f\"  Min confidence: {result['confidences'].min():.3f}\")\n",
    "    print(f\"  Max confidence: {result['confidences'].max():.3f}\")\n",
    "\n",
    "print(f\"\\n\\nAll {len(expert_results)} experts have completed predictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 9. Visualize Expert Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show each expert's predicted grid and confidence\n",
    "n_experts = len(expert_results)\n",
    "\n",
    "fig, axes = plt.subplots(n_experts, 3, figsize=(12, 4 * n_experts))\n",
    "if n_experts == 1:\n",
    "    axes = axes.reshape(1, 3)\n",
    "\n",
    "# Get ground truth if available\n",
    "ground_truth = None\n",
    "if task['test'] and 'output' in task['test'][0]:\n",
    "    ground_truth = np.array(task['test'][0]['output'])\n",
    "\n",
    "for idx, (model_name, result) in enumerate(expert_results.items()):\n",
    "    # Predicted grid\n",
    "    plot_grid(result['grid'], f\"{model_name}\\nPredicted Grid\", axes[idx, 0], show_values=True)\n",
    "    \n",
    "    # Confidence heatmap\n",
    "    im = plot_confidence_heatmap(result['confidences'], f\"{model_name}\\nConfidence\", axes[idx, 1])\n",
    "    \n",
    "    # Ground truth comparison (if available)\n",
    "    if ground_truth is not None:\n",
    "        plot_grid(ground_truth, \"Ground Truth\", axes[idx, 2], show_values=True)\n",
    "        # Check accuracy\n",
    "        correct = np.sum(result['grid'] == ground_truth)\n",
    "        total = ground_truth.size\n",
    "        accuracy = correct / total\n",
    "        axes[idx, 2].set_xlabel(f\"Accuracy: {correct}/{total} = {accuracy:.1%}\")\n",
    "    else:\n",
    "        axes[idx, 2].text(0.5, 0.5, \"No ground truth\", ha='center', va='center')\n",
    "        axes[idx, 2].set_title(\"Ground Truth\")\n",
    "\n",
    "plt.suptitle(f\"Expert Predictions for Task: {task['task_id']}\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": "## 10. View Expert Reasoning (if mode='reasoning')"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": "## 10. View Expert Reasoning (if mode='reasoning')"
  },
  {
   "cell_type": "code",
   "id": "qosxer0ourc",
   "source": "# View the raw output / reasoning from each expert\nfor expert_name, result in expert_results.items():\n    print(f\"\\n{'='*70}\")\n    print(f\"Expert: {expert_name}\")\n    print(f\"{'='*70}\")\n    \n    if 'raw_output' in result:\n        # whole_grid mode\n        print(\"\\nRaw generated output:\")\n        print(\"-\" * 50)\n        print(result['raw_output'])\n        print(\"-\" * 50)\n        \n    elif 'reasoning' in result:\n        # cell_reasoning mode\n        print(\"\\nPer-cell reasoning:\")\n        for item in result['reasoning'][:3]:  # Show first 3 cells\n            cell = item['cell']\n            text = item['text']\n            predicted = result['grid'][cell[0], cell[1]]\n            print(f\"\\n  Cell {cell} -> {predicted}:\")\n            print(f\"    {text[:200]}...\")\n        if len(result['reasoning']) > 3:\n            print(f\"\\n  ... and {len(result['reasoning']) - 3} more cells\")\n    else:\n        print(\"\\n  (direct mode - no reasoning available)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": "## 11. Compare Probability Distributions Across Experts"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select which expert to visualize\n",
    "selected_expert = list(expert_results.keys())[0]  # First expert\n",
    "result = expert_results[selected_expert]\n",
    "\n",
    "h, w = target_size\n",
    "fig, axes = plt.subplots(h, w, figsize=(4 * w, 3 * h))\n",
    "if h == 1 and w == 1:\n",
    "    axes = np.array([[axes]])\n",
    "elif h == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "elif w == 1:\n",
    "    axes = axes.reshape(-1, 1)\n",
    "\n",
    "for i in range(h):\n",
    "    for j in range(w):\n",
    "        ax = axes[i, j]\n",
    "        probs = result['probabilities'][i, j]\n",
    "        \n",
    "        ax.bar(range(10), probs, color=ARC_COLORS, edgecolor='black', linewidth=0.3)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.set_xticks(range(10))\n",
    "        ax.set_xticklabels(range(10), fontsize=7)\n",
    "        ax.set_title(f'({i},{j}) → {np.argmax(probs)}', fontsize=9)\n",
    "        ax.tick_params(axis='y', labelsize=7)\n",
    "\n",
    "plt.suptitle(f\"{selected_expert}: Per-Cell Probability Distributions\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## 12. Expert Agreement Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze where experts agree and disagree\n",
    "if len(expert_results) > 1:\n",
    "    predictions = np.stack([r['grid'] for r in expert_results.values()])\n",
    "    \n",
    "    # Count agreements\n",
    "    h, w = target_size\n",
    "    agreement_map = np.zeros((h, w))\n",
    "    \n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            cell_preds = predictions[:, i, j]\n",
    "            unique_preds = len(np.unique(cell_preds))\n",
    "            # 1 = all agree, 0 = all different\n",
    "            agreement_map[i, j] = 1.0 - (unique_preds - 1) / (len(expert_results) - 1)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    \n",
    "    # Agreement heatmap\n",
    "    im = axes[0].imshow(agreement_map, cmap='RdYlGn', vmin=0, vmax=1, interpolation='nearest')\n",
    "    axes[0].set_title('Expert Agreement\\n(green = all agree, red = all different)')\n",
    "    plt.colorbar(im, ax=axes[0])\n",
    "    \n",
    "    # Show values\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            preds = [str(int(predictions[k, i, j])) for k in range(len(expert_results))]\n",
    "            axes[0].text(j, i, '/'.join(preds), ha='center', va='center', fontsize=8)\n",
    "    \n",
    "    # Summary stats\n",
    "    total_cells = h * w\n",
    "    full_agreement = np.sum(agreement_map == 1.0)\n",
    "    partial_agreement = np.sum((agreement_map > 0) & (agreement_map < 1))\n",
    "    no_agreement = np.sum(agreement_map == 0)\n",
    "    \n",
    "    labels = ['Full Agreement', 'Partial', 'No Agreement']\n",
    "    sizes = [full_agreement, partial_agreement, no_agreement]\n",
    "    colors_pie = ['#2ECC40', '#FFDC00', '#FF4136']\n",
    "    \n",
    "    axes[1].pie(sizes, labels=labels, colors=colors_pie, autopct='%1.0f%%', startangle=90)\n",
    "    axes[1].set_title(f'Agreement Summary\\n({total_cells} cells total)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Need at least 2 experts for agreement analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## 13. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unload models to free memory\n",
    "llm_manager.unload()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"GPU memory freed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}